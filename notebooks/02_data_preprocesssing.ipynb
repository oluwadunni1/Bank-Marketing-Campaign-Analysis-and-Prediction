{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3844899-8115-472d-baeb-69f71c9108bb",
   "metadata": {},
   "source": [
    "## **Data Preprocessing and Feature Engineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10acd00-839b-477e-b918-d4dc21921130",
   "metadata": {},
   "source": [
    "This notebook continues from the Exploratory Data Analysis (EDA) phase of the Bank Marketing prediction project. Having gained an understanding of the dataset’s structure, data quality, and key relationships, the focus now shifts to preparing the data for machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbc61f8-0021-40bb-a119-13474e634d82",
   "metadata": {},
   "source": [
    "The objectives are to:\n",
    "\n",
    "- Encode categorical variables into numerical representations\n",
    "\n",
    "- Scale or normalize numeric features for model compatibility\n",
    "\n",
    "- Apply transformations to correct feature skewness where necessary\n",
    "\n",
    "- Split the dataset into training and testing subsets for fair evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a649fb38-f62a-4057-8e63-37ee7b63c976",
   "metadata": {},
   "source": [
    "### Import Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58272d32-f64b-47e3-83d6-13e008111ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# File path handling\n",
    "from pathlib import Path\n",
    "\n",
    "# Machine learning - preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import (\n",
    "    PowerTransformer, \n",
    "    OrdinalEncoder, \n",
    "    OneHotEncoder, \n",
    "    StandardScaler\n",
    ")\n",
    "\n",
    "# Model persistence\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d74d9e3-f86a-40c2-9846-0308be341307",
   "metadata": {},
   "source": [
    "### Load dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d3d60eb-a4d2-414f-8d21-78e113e13ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dataset loaded successfully from: C:\\Users\\hp\\Documents\\DA projects\\Bank Marketing Prediction\\data\\raw\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>technician</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>7</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>25</td>\n",
       "      <td>aug</td>\n",
       "      <td>117</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>514</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>18</td>\n",
       "      <td>jun</td>\n",
       "      <td>185</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>36</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>602</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>14</td>\n",
       "      <td>may</td>\n",
       "      <td>111</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "      <td>student</td>\n",
       "      <td>single</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>34</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>28</td>\n",
       "      <td>may</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>26</td>\n",
       "      <td>technician</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>889</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>3</td>\n",
       "      <td>feb</td>\n",
       "      <td>902</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  age          job  marital  education default  balance housing loan  \\\n",
       "0   0   42   technician  married  secondary      no        7      no   no   \n",
       "1   1   38  blue-collar  married  secondary      no      514      no   no   \n",
       "2   2   36  blue-collar  married  secondary      no      602     yes   no   \n",
       "3   3   27      student   single  secondary      no       34     yes   no   \n",
       "4   4   26   technician  married  secondary      no      889     yes   no   \n",
       "\n",
       "    contact  day month  duration  campaign  pdays  previous poutcome  y  \n",
       "0  cellular   25   aug       117         3     -1         0  unknown  0  \n",
       "1   unknown   18   jun       185         1     -1         0  unknown  0  \n",
       "2   unknown   14   may       111         2     -1         0  unknown  0  \n",
       "3   unknown   28   may        10         2     -1         0  unknown  0  \n",
       "4  cellular    3   feb       902         1     -1         0  unknown  1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move one level up (to the project root), then into data/raw\n",
    "data_dir = Path().resolve().parent / \"data\" / \"raw\"\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(data_dir / \"train.csv\")\n",
    "\n",
    "print(f\" Dataset loaded successfully from: {data_dir}\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e1ce21-d48d-488e-aa70-7c601ba83d62",
   "metadata": {},
   "source": [
    "### Feature Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78016008-a969-47f8-b7f1-d9673f1363a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'id' column since it carries no predictive information\n",
    "df = df.drop(columns=['id'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7c98f8-df60-48d6-afbf-97b6a1a3dd5d",
   "metadata": {},
   "source": [
    "#### Handling the `pdays` variable \n",
    "\n",
    "pdays represents the number of days since the client was last contacted, with -1 indicating no prior contact.\n",
    "Leaving -1 as-is could mislead models, so we’ll decompose it into two separate variables:\n",
    "\n",
    "Transformation Logic:\n",
    "\n",
    "- was_contacted_before: binary flag → 1 if pdays != -1, else 0\n",
    "\n",
    "- log_pdays: log-transformed version of pdays (applied only where pdays > 0)\n",
    "\n",
    "    - Assign 0 to clients who were never contacted (pdays == -1)\n",
    "\n",
    "Finally, we’ll drop the original pdays column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de89e7f5-95f9-4dfc-9dce-f1cf8574f241",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>was_contacted_before</th>\n",
       "      <th>log_pdays</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   was_contacted_before  log_pdays\n",
       "0                     0        0.0\n",
       "1                     0        0.0\n",
       "2                     0        0.0\n",
       "3                     0        0.0\n",
       "4                     0        0.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: Special Feature Treatment - pdays\n",
    "\n",
    "# Create binary flag\n",
    "df['was_contacted_before'] = np.where(df['pdays'] != -1, 1, 0)\n",
    "\n",
    "# Safely compute log-transformed pdays\n",
    "with np.errstate(divide='ignore'):\n",
    "    df['log_pdays'] = np.where(\n",
    "        df['pdays'] > 0,\n",
    "        np.log(df['pdays'] + 1),\n",
    "        0\n",
    "    )\n",
    "\n",
    "# Drop original column\n",
    "df.drop(columns='pdays', inplace=True)\n",
    "\n",
    "# Quick check\n",
    "df[['was_contacted_before', 'log_pdays']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df51b30-5c2b-4db3-809c-4995afdaa933",
   "metadata": {},
   "source": [
    "#### Train - Test Split "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2d852f-0112-48ff-b48f-3683b4d3ae93",
   "metadata": {},
   "source": [
    "To ensure unbiased model evaluation and prevent data leakage, the dataset is divided into training and testing subsets using stratified sampling based on the target variable (y).\n",
    "This preserves the proportion of positive and negative classes in both sets.\n",
    "The training set will be used to fit all preprocessing transformers (e.g., scaling, encoding, and Yeo–Johnson), which will then be applied consistently to the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31d0163a-c422-4770-b138-cb0cafc3ebe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (600000, 17)\n",
      "Test set shape: (150000, 17)\n",
      "Target distribution in training set:\n",
      "y\n",
      "0    0.87935\n",
      "1    0.12065\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Target distribution in test set:\n",
      "y\n",
      "0    0.879347\n",
      "1    0.120653\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Separate features and target\n",
    "X = df.drop(columns=['y'])\n",
    "y = df['y']\n",
    "\n",
    "# Stratified split to maintain class balance\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,             # 80/20 split\n",
    "    stratify=y,                # preserve target distribution\n",
    "    random_state=42            # ensure reproducibility\n",
    ")\n",
    "\n",
    "# Check resulting shapes\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)\n",
    "print(\"Target distribution in training set:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(\"\\nTarget distribution in test set:\")\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa49a356-a272-436e-ac61-72cddf369871",
   "metadata": {},
   "source": [
    "### Dual Preprocessing Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843a4202-f123-4d6e-8611-2b661895300b",
   "metadata": {},
   "source": [
    "To effectively compare the performance of distinct model families, a tree-based model (XGBoost) and a gradient-descent model (Neural Network), two separate, tailored preprocessing workflows were implemented. Each pipeline was designed to meet the fundamental input requirements of its respective algorithm. Critically, both were fitted only on the training data to ensure sound methodological practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac6f7b7-9956-413f-bae7-620b77ecd7a7",
   "metadata": {},
   "source": [
    "#### The XGBoost Pipeline (Optimized for Trees)\n",
    "\n",
    "Tree-based models are robust to feature magnitude and prefer rank-based information. This pipeline focused on creating density and reducing skew without scaling:\n",
    "\n",
    "- Numeric Features: The Yeo–Johnson transformation was applied to mitigate skewness, but standard scaling was skipped as it provides no performance benefit for tree ensembles.\n",
    "\n",
    "- Categorical Features: Ordinal Encoding was utilized, converting categories into integers for computational efficiency, a format trees handle naturally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3fb33382-5cf7-48a8-baf7-c9832c4bbb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create copies for XGBoost\n",
    "X_train_xgb = X_train.copy()\n",
    "X_test_xgb = X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b389d7d-af8e-4c13-be5d-cfcabf36c5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Feature Types Identified:\n",
      "   Numeric: 7 features\n",
      "   Categorical: 9 features\n",
      "   Binary: 1 features\n"
     ]
    }
   ],
   "source": [
    "numeric_cols_xgb = [\n",
    "    'balance', 'duration', 'age', 'campaign', \n",
    "    'previous', 'log_pdays', 'day'\n",
    "]\n",
    "binary_cols_xgb = ['was_contacted_before']\n",
    "\n",
    "categorical_cols_xgb = [\n",
    "    col for col in X_train_xgb.columns \n",
    "    if col not in numeric_cols_xgb + binary_cols_xgb\n",
    "]\n",
    "\n",
    "print(f\"\\n Feature Types Identified:\")\n",
    "print(f\"   Numeric: {len(numeric_cols_xgb)} features\")\n",
    "print(f\"   Categorical: {len(categorical_cols_xgb)} features\")\n",
    "print(f\"   Binary: {len(binary_cols_xgb)} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c55d115-aef7-44e8-847c-17411b1bb1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Applying Power Transform to skewed features...\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n Applying Power Transform to skewed features...\")\n",
    "\n",
    "pt_xgb = PowerTransformer(method='yeo-johnson', standardize=False)\n",
    "num_skewed_xgb = ['balance', 'duration']\n",
    "\n",
    "X_train_xgb[num_skewed_xgb] = pt_xgb.fit_transform(X_train_xgb[num_skewed_xgb])\n",
    "X_test_xgb[num_skewed_xgb] = pt_xgb.transform(X_test_xgb[num_skewed_xgb])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d19e7b5f-6ff8-4de4-8473-81efcfccb6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " XGBoost transformer saved successfully at: C:\\Users\\hp\\Documents\\DA projects\\Bank Marketing Prediction\\transformers\\xgb_yeo_johnson_transformer.pkl\n"
     ]
    }
   ],
   "source": [
    "# Save transformer\n",
    "\n",
    "# Create path to project root\n",
    "project_root = Path().resolve().parent\n",
    "\n",
    "# Create /transformers directory inside project root \n",
    "transformers_dir = project_root / \"transformers\"\n",
    "transformers_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "xgb_transformer_path = transformers_dir / \"xgb_yeo_johnson_transformer.pkl\"\n",
    "joblib.dump(pt_xgb, xgb_transformer_path)\n",
    "print(f\" XGBoost transformer saved successfully at: {xgb_transformer_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7504aa6e-0872-4291-b2e6-eebeeb96ec45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Applying Ordinal Encoding...\n",
      " XGBoost Ordinal Encoder saved successfully at: C:\\Users\\hp\\Documents\\DA projects\\Bank Marketing Prediction\\encoders\\xgb_ordinal_encoder.pkl\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n Applying Ordinal Encoding...\")\n",
    "\n",
    "ordinal_encoder_xgb = OrdinalEncoder(\n",
    "    handle_unknown='use_encoded_value',\n",
    "    unknown_value=-1\n",
    ")\n",
    "\n",
    "X_train_xgb[categorical_cols_xgb] = ordinal_encoder_xgb.fit_transform(\n",
    "    X_train_xgb[categorical_cols_xgb]\n",
    ")\n",
    "X_test_xgb[categorical_cols_xgb] = ordinal_encoder_xgb.transform(\n",
    "    X_test_xgb[categorical_cols_xgb]\n",
    ")\n",
    "\n",
    "# Create /encoders directory at project root \n",
    "encoders_dir = project_root / \"encoders\"\n",
    "encoders_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Save encoder ---\n",
    "xgb_encoder_path = encoders_dir / \"xgb_ordinal_encoder.pkl\"\n",
    "joblib.dump(ordinal_encoder_xgb, xgb_encoder_path)\n",
    "\n",
    "print(f\" XGBoost Ordinal Encoder saved successfully at: {xgb_encoder_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e29e7da2-9ed8-4de5-873a-5b388387b605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " XGBoost Preprocessing Complete!\n",
      "   X_train_xgb: (600000, 17)\n",
      "   X_test_xgb:  (150000, 17)\n",
      "   Total features: 17\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n XGBoost Preprocessing Complete!\")\n",
    "print(f\"   X_train_xgb: {X_train_xgb.shape}\")\n",
    "print(f\"   X_test_xgb:  {X_test_xgb.shape}\")\n",
    "print(f\"   Total features: {X_train_xgb.shape[1]}\")\n",
    "\n",
    "# Sanity check\n",
    "assert X_train_xgb.shape[0] == y_train.shape[0], \" Train shape mismatch!\"\n",
    "assert X_test_xgb.shape[0] == y_test.shape[0], \" Test shape mismatch!\"\n",
    "assert X_train_xgb.isnull().sum().sum() == 0, \" NaN values found in train!\"\n",
    "assert X_test_xgb.isnull().sum().sum() == 0, \" NaN values found in test!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0bb681ba-50ef-4e0b-b0e6-cd90de2f27ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Class Distribution (Training):\n",
      "   Class 0 (No):  87.94%\n",
      "   Class 1 (Yes): 12.06%\n",
      "   Imbalance ratio: 7.29\n"
     ]
    }
   ],
   "source": [
    "# Check class distribution\n",
    "class_dist = y_train.value_counts(normalize=True)\n",
    "print(f\"\\n Class Distribution (Training):\")\n",
    "print(f\"   Class 0 (No):  {class_dist[0]:.2%}\")\n",
    "print(f\"   Class 1 (Yes): {class_dist[1]:.2%}\")\n",
    "\n",
    "# Calculate imbalance ratio for scale_pos_weight\n",
    "imbalance_ratio = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "print(f\"   Imbalance ratio: {imbalance_ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dcc1ba6a-d1b2-4b10-ad1a-dea7dd6f6f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed XGBoost datasets saved in: C:\\Users\\hp\\Documents\\DA projects\\Bank Marketing Prediction\\data\\processed\n"
     ]
    }
   ],
   "source": [
    "# Create a 'data/processed' folder at the project root\n",
    "processed_dir = project_root / \"data\" / \"processed\"\n",
    "processed_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save datasets to the project root's processed folder\n",
    "joblib.dump(X_train_xgb, processed_dir / \"X_train_xgb.pkl\")\n",
    "joblib.dump(X_test_xgb, processed_dir / \"X_test_xgb.pkl\")\n",
    "joblib.dump(y_train, processed_dir / \"y_train.pkl\")\n",
    "joblib.dump(y_test, processed_dir / \"y_test.pkl\")\n",
    "\n",
    "print(f\"Processed XGBoost datasets saved in: {processed_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d21b9c5-1806-453b-a2f1-75d6991eb5dc",
   "metadata": {},
   "source": [
    "### The Neural Network Pipeline (Optimized for Stability)\n",
    "\n",
    "\n",
    "The Neural Network Pipeline (Optimized for Stability)\n",
    "\n",
    " Neural Networks require standardized inputs and bounded ranges for fast, stable training via backpropagation:\n",
    "\n",
    "- Numeric Features: Features first received the Yeo–Johnson transformation for normalization, followed by Standardization (mean 0, variance 1). Finally, outlier clipping was applied to restrict the range (e.g., to $\\pm 5$ standard deviations) to prevent exploding gradients.\n",
    "\n",
    "- Categorical Features: One-Hot Encoding (OHE) was necessary to avoid introducing false numerical relationships between categories, which would confuse the network's distance calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "96dae2b0-5fd2-4f94-ab76-c78009bf5079",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create copies for Neural networks\n",
    "X_train_nn = X_train.copy()\n",
    "X_test_nn = X_test.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "23f8a2f4-aeed-4551-8b0e-dae982b8a490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Types Identified:\n",
      "   Numeric: 7 features\n",
      "   Categorical: 9 features\n",
      "   Binary: 1 features\n"
     ]
    }
   ],
   "source": [
    "# Identify Feature Types\n",
    "numeric_cols_nn = [\n",
    "    'balance', 'duration', 'age', 'campaign', \n",
    "    'previous', 'log_pdays', 'day'\n",
    "]\n",
    "binary_cols_nn = ['was_contacted_before']\n",
    "\n",
    "categorical_cols_nn = [\n",
    "    col for col in X_train_nn.columns \n",
    "    if col not in numeric_cols_nn + binary_cols_nn\n",
    "]\n",
    "\n",
    "print(f\"\\nFeature Types Identified:\")\n",
    "print(f\"   Numeric: {len(numeric_cols_nn)} features\")\n",
    "print(f\"   Categorical: {len(categorical_cols_nn)} features\")\n",
    "print(f\"   Binary: {len(binary_cols_nn)} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "612227ab-fe62-48a6-b63d-3747b43cd18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Applying Power Transform with standardization...\n",
      " Neural Network transformer saved successfully at: C:\\Users\\hp\\Documents\\DA projects\\Bank Marketing Prediction\\transformers\\nn_yeo_johnson_transformer.pkl\n"
     ]
    }
   ],
   "source": [
    "# Power Transform Skewed Numeric Features\n",
    "\n",
    "print(f\"\\n Applying Power Transform with standardization...\")\n",
    "\n",
    "pt_nn = PowerTransformer(method='yeo-johnson', standardize=True)\n",
    "num_skewed_nn = ['balance', 'duration']\n",
    "\n",
    "X_train_nn[num_skewed_nn] = pt_nn.fit_transform(X_train_nn[num_skewed_nn])\n",
    "X_test_nn[num_skewed_nn] = pt_nn.transform(X_test_nn[num_skewed_nn])\n",
    "\n",
    "# Save transformer\n",
    "nn_transformer_path = transformers_dir / \"nn_yeo_johnson_transformer.pkl\"\n",
    "joblib.dump(pt_nn, nn_transformer_path)\n",
    "print(f\" Neural Network transformer saved successfully at: {nn_transformer_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b3dbcd5e-3edc-49cd-b353-3b3e83fba3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Applying One-Hot Encoding...\n",
      " Neural Network One-Hot Encoder saved successfully at: C:\\Users\\hp\\Documents\\DA projects\\Bank Marketing Prediction\\encoders\\nn_onehot_encoder.pkl\n",
      "   Created 44 one-hot encoded features\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n Applying One-Hot Encoding...\")\n",
    "\n",
    "# Initialize OneHotEncoder\n",
    "onehot_encoder_nn = OneHotEncoder(\n",
    "    handle_unknown='ignore',\n",
    "    sparse_output=False\n",
    ")\n",
    "\n",
    "# Fit and transform\n",
    "X_train_cat_ohe = onehot_encoder_nn.fit_transform(X_train_nn[categorical_cols_nn])\n",
    "X_test_cat_ohe = onehot_encoder_nn.transform(X_test_nn[categorical_cols_nn])\n",
    "\n",
    "# Convert to DataFrames\n",
    "ohe_feature_names = onehot_encoder_nn.get_feature_names_out(categorical_cols_nn)\n",
    "X_train_cat_ohe = pd.DataFrame(\n",
    "    X_train_cat_ohe, \n",
    "    columns=ohe_feature_names, \n",
    "    index=X_train_nn.index\n",
    ")\n",
    "X_test_cat_ohe = pd.DataFrame(\n",
    "    X_test_cat_ohe, \n",
    "    columns=ohe_feature_names, \n",
    "    index=X_test_nn.index\n",
    ")\n",
    "\n",
    "# Drop original categorical columns and concatenate one-hot encoded\n",
    "X_train_nn = pd.concat(\n",
    "    [X_train_nn.drop(columns=categorical_cols_nn), X_train_cat_ohe], \n",
    "    axis=1\n",
    ")\n",
    "X_test_nn = pd.concat(\n",
    "    [X_test_nn.drop(columns=categorical_cols_nn), X_test_cat_ohe], \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Align columns (ensure test has same columns as train)\n",
    "X_test_nn = X_test_nn.reindex(columns=X_train_nn.columns, fill_value=0)\n",
    "\n",
    "# --- Create /encoders directory at project root (reuse if already exists) ---\n",
    "encoders_dir = project_root / \"encoders\"\n",
    "encoders_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Save One-Hot Encoder ---\n",
    "nn_encoder_path = encoders_dir / \"nn_onehot_encoder.pkl\"\n",
    "joblib.dump(onehot_encoder_nn, nn_encoder_path)\n",
    "\n",
    "print(f\" Neural Network One-Hot Encoder saved successfully at: {nn_encoder_path}\")\n",
    "print(f\"   Created {len(ohe_feature_names)} one-hot encoded features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "51e0f43c-d717-4248-972a-29ae5ee72e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Applying Standard Scaling to numeric features...\n",
      " Neural Network Standard Scaler saved successfully at: C:\\Users\\hp\\Documents\\DA projects\\Bank Marketing Prediction\\scalers\\nn_standard_scaler.pkl\n",
      "   Clipped values to range: [-5, 5]\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n Applying Standard Scaling to numeric features...\")\n",
    "\n",
    "scaler_nn = StandardScaler()\n",
    "\n",
    "# Scale all numeric columns (not binary)\n",
    "X_train_nn[numeric_cols_nn] = scaler_nn.fit_transform(X_train_nn[numeric_cols_nn])\n",
    "X_test_nn[numeric_cols_nn] = scaler_nn.transform(X_test_nn[numeric_cols_nn])\n",
    "\n",
    "# Optional: Clip extreme values to prevent gradient explosion\n",
    "CLIP_VALUE = 5\n",
    "X_train_nn[numeric_cols_nn] = X_train_nn[numeric_cols_nn].clip(-CLIP_VALUE, CLIP_VALUE)\n",
    "X_test_nn[numeric_cols_nn] = X_test_nn[numeric_cols_nn].clip(-CLIP_VALUE, CLIP_VALUE)\n",
    "\n",
    "# --- Create /scalers directory at project root ---\n",
    "scalers_dir = project_root / \"scalers\"\n",
    "scalers_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Save Scaler ---\n",
    "scaler_path = scalers_dir / \"nn_standard_scaler.pkl\"\n",
    "joblib.dump(scaler_nn, scaler_path)\n",
    "\n",
    "print(f\" Neural Network Standard Scaler saved successfully at: {scaler_path}\")\n",
    "print(f\"   Clipped values to range: [{-CLIP_VALUE}, {CLIP_VALUE}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d3bc3fcf-3b9b-4c51-85dc-63c8ed4868f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Neural Network Preprocessing Complete!\n",
      "   X_train_nn: (600000, 52)\n",
      "   X_test_nn:  (150000, 52)\n",
      "   Total features: 52\n"
     ]
    }
   ],
   "source": [
    "#Final Neural Network Dataset Check\n",
    "\n",
    "print(f\"\\n Neural Network Preprocessing Complete!\")\n",
    "print(f\"   X_train_nn: {X_train_nn.shape}\")\n",
    "print(f\"   X_test_nn:  {X_test_nn.shape}\")\n",
    "print(f\"   Total features: {X_train_nn.shape[1]}\")\n",
    "\n",
    "# Sanity checks\n",
    "assert X_train_nn.shape[0] == y_train.shape[0], \"Train shape mismatch!\"\n",
    "assert X_test_nn.shape[0] == y_test.shape[0], \" Test shape mismatch!\"\n",
    "assert X_train_nn.isnull().sum().sum() == 0, \" NaN values found in train!\"\n",
    "assert X_test_nn.isnull().sum().sum() == 0, \" NaN values found in test!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fe19ca46-e247-4df8-bb40-4103d31a544c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processed Neural Network datasets saved successfully in: C:\\Users\\hp\\Documents\\DA projects\\Bank Marketing Prediction\\data\\processed\n"
     ]
    }
   ],
   "source": [
    "# Save Neural Network datasets to project root's processed folder \n",
    "joblib.dump(X_train_nn, processed_dir / \"X_train_nn.pkl\")\n",
    "joblib.dump(X_test_nn, processed_dir / \"X_test_nn.pkl\")\n",
    "joblib.dump(y_train, processed_dir / \"y_train.pkl\")  # already saved for XGBoost, can overwrite safely\n",
    "joblib.dump(y_test, processed_dir / \"y_test.pkl\")    # already saved for XGBoost, can overwrite safely\n",
    "\n",
    "print(f\" Processed Neural Network datasets saved successfully in: {processed_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813c455c-652c-497e-b312-246f0e068fec",
   "metadata": {},
   "source": [
    "With our data preprocessing pipeline complete, we have successfully prepared two optimized datasets tailored for different modeling approaches. The XGBoost dataset utilizes ordinal encoding and selective power transformation to preserve tree-based interpretability, while the Neural Network dataset employs one-hot encoding, standardization, and clipping to ensure stable gradient flow."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
